{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAKE_SUBMISSION = True          # Generate output file.\n",
    "CV_ONLY = False                 # Do validation only; do not generate predicitons.\n",
    "FIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\n",
    "FIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\n",
    "FIT_COMBINED_TRAIN_SET = True   # Fit combined 2016-2017 training set\n",
    "USE_SEASONAL_FEATURES = True\n",
    "VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n",
    "LEARNING_RATE = 0.007           # shrinkage rate for boosting roudns\n",
    "ROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\n",
    "OPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\n",
    "FUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renmeng/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/renmeng/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties16 = pd.read_csv('../data/properties_2016.csv/properties_2016.csv', low_memory = False)\n",
    "# properties17 = pd.read_csv('../data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "# Number of properties in the zip\n",
    "zip_count = properties16['regionidzip'].value_counts().to_dict()\n",
    "# Number of properties in the city\n",
    "city_count = properties16['regionidcity'].value_counts().to_dict()\n",
    "# Median year of construction by neighborhood\n",
    "medyear = properties16.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n",
    "# Mean square feet by neighborhood\n",
    "meanarea = properties16.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n",
    "# Neighborhood latitude and longitude\n",
    "medlat = properties16.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n",
    "medlong = properties16.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n",
    "\n",
    "train = pd.read_csv(\"../data/train_2016_v2.csv/train_2016_v2.csv\")\n",
    "for c in properties16.columns:\n",
    "    properties16[c]=properties16[c].fillna(-1)\n",
    "    if properties16[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties16[c].values))\n",
    "        properties16[c] = lbl.transform(list(properties16[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train.merge(properties16, how='left', on='parcelid')\n",
    "#2016-09-15\n",
    "select_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE\n",
    "if USE_SEASONAL_FEATURES:\n",
    "    basedate = pd.to_datetime('2015-11-15').toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs to features that depend on target variable\n",
    "# (Ideally these should be recalculated, and the dependent features recalculated,\n",
    "#  when fitting to the full training set.  But I haven't implemented that yet.)\n",
    "\n",
    "# Standard deviation of target value for properties in the city/zip/neighborhood\n",
    "citystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "zipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "hoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90275, 60)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train.merge(properties16, how='left', on='parcelid')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_features(df):\n",
    "    # Nikunj's features\n",
    "    # Number of properties in the zip\n",
    "    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n",
    "    # Number of properties in the city\n",
    "    df['N-city_count'] = df['regionidcity'].map(city_count)\n",
    "    # Does property have a garage, pool or hot tub and AC?\n",
    "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n",
    "                         (df['pooltypeid10']>0) & \\\n",
    "                         (df['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "    # More features\n",
    "    # Mean square feet of neighborhood properties\n",
    "    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n",
    "    # Median year of construction of neighborhood properties\n",
    "    df['med_year'] = df['regionidneighborhood'].map(medyear)\n",
    "    # Neighborhood latitude and longitude\n",
    "    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n",
    "    df['med_long'] = df['regionidneighborhood'].map(medlong)\n",
    "\n",
    "    df['zip_std'] = df['regionidzip'].map(zipstd)\n",
    "    df['city_std'] = df['regionidcity'].map(citystd)\n",
    "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n",
    "    \n",
    "    if USE_SEASONAL_FEATURES:\n",
    "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.cos)\n",
    "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.to_datetime('2016-01-01').toordinal()-pd.to_datetime('2015-11-15').toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8085139204310487"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp * (2 * np.pi/365.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n",
    "            'buildingqualitytypeid', 'regionidcity']\n",
    "droptrain = ['parcelid', 'logerror', 'transactiondate']\n",
    "droptest = ['ParcelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'parcelid', u'logerror', u'transactiondate', u'airconditioningtypeid',\n",
       "       u'architecturalstyletypeid', u'basementsqft', u'bathroomcnt',\n",
       "       u'bedroomcnt', u'buildingclasstypeid', u'buildingqualitytypeid',\n",
       "       u'calculatedbathnbr', u'decktypeid', u'finishedfloor1squarefeet',\n",
       "       u'calculatedfinishedsquarefeet', u'finishedsquarefeet12',\n",
       "       u'finishedsquarefeet13', u'finishedsquarefeet15',\n",
       "       u'finishedsquarefeet50', u'finishedsquarefeet6', u'fips',\n",
       "       u'fireplacecnt', u'fullbathcnt', u'garagecarcnt', u'garagetotalsqft',\n",
       "       u'hashottuborspa', u'heatingorsystemtypeid', u'latitude', u'longitude',\n",
       "       u'lotsizesquarefeet', u'poolcnt', u'poolsizesum', u'pooltypeid10',\n",
       "       u'pooltypeid2', u'pooltypeid7', u'propertycountylandusecode',\n",
       "       u'propertylandusetypeid', u'propertyzoningdesc',\n",
       "       u'rawcensustractandblock', u'regionidcity', u'regionidcounty',\n",
       "       u'regionidneighborhood', u'regionidzip', u'roomcnt', u'storytypeid',\n",
       "       u'threequarterbathnbr', u'typeconstructiontypeid', u'unitcnt',\n",
       "       u'yardbuildingsqft17', u'yardbuildingsqft26', u'yearbuilt',\n",
       "       u'numberofstories', u'fireplaceflag', u'structuretaxvaluedollarcnt',\n",
       "       u'taxvaluedollarcnt', u'assessmentyear', u'landtaxvaluedollarcnt',\n",
       "       u'taxamount', u'taxdelinquencyflag', u'taxdelinquencyyear',\n",
       "       u'censustractandblock'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape full training set: (90275, 72)\n",
      "Dropped vars: 7\n",
      "Shape valid X: (14304, 65)\n",
      "Shape valid y: (14304,)\n",
      "\n",
      "Full training set after removing outliers, before dropping vars:\n",
      "Shape training set: (88528, 72)\n",
      "\n",
      "Training subset after removing outliers:\n",
      "Shape train X: (74478, 65)\n",
      "Shape train y: (74478,)\n",
      "\n",
      "Full trainng set:\n",
      "Shape train X: (74478, 65)\n",
      "Shape train y: (74478,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renmeng/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "calculate_features(train_df)\n",
    "\n",
    "x_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\n",
    "y_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n",
    "\n",
    "print('Shape full training set: {}'.format(train_df.shape))\n",
    "print('Dropped vars: {}'.format(len(dropvars+droptrain)))\n",
    "print('Shape valid X: {}'.format(x_valid.shape))\n",
    "print('Shape valid y: {}'.format(y_valid.shape))\n",
    "\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "print('\\nFull training set after removing outliers, before dropping vars:')     \n",
    "print('Shape training set: {}\\n'.format(train_df.shape))\n",
    "\n",
    "if FIT_FULL_TRAIN_SET:\n",
    "    full_train = train_df.copy()\n",
    "\n",
    "train_df=train_df[~select_qtr4]\n",
    "x_train=train_df.drop(dropvars+droptrain, axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "n_train = x_train.shape[0]\n",
    "print('Training subset after removing outliers:')     \n",
    "print('Shape train X: {}'.format(x_train.shape))\n",
    "print('Shape train y: {}'.format(y_train.shape))\n",
    "\n",
    "if FIT_FULL_TRAIN_SET:\n",
    "    x_full = full_train.drop(dropvars+droptrain, axis=1)\n",
    "    y_full = full_train[\"logerror\"].values.astype(np.float32)\n",
    "    n_full = x_full.shape[0]\n",
    "    print('\\nFull trainng set:')     \n",
    "    print('Shape train X: {}'.format(x_train.shape))\n",
    "    print('Shape train y: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test: (2985217, 65)\n"
     ]
    }
   ],
   "source": [
    "if not CV_ONLY:\n",
    "    # Generate test set data\n",
    "    \n",
    "    sample_submission = pd.read_csv('../data/sample_submission.csv', low_memory = False)\n",
    "    \n",
    "    # Process properties for 2016\n",
    "    test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                        how = 'left', on = 'ParcelId' )\n",
    "    if USE_SEASONAL_FEATURES:\n",
    "        test_df['transactiondate'] = '2016-10-31'\n",
    "        droptest += ['transactiondate']\n",
    "    calculate_features(test_df)\n",
    "    x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "    print('Shape test: {}'.format(x_test.shape))\n",
    "\n",
    "    # Process properties for 2017\n",
    "#     for c in properties17.columns:\n",
    "#         properties17[c]=properties17[c].fillna(-1)\n",
    "#         if properties17[c].dtype == 'object':\n",
    "#             lbl = LabelEncoder()\n",
    "#             lbl.fit(list(properties17[c].values))\n",
    "#             properties17[c] = lbl.transform(list(properties17[c].values))\n",
    "#     zip_count = properties17['regionidzip'].value_counts().to_dict()\n",
    "#     city_count = properties17['regionidcity'].value_counts().to_dict()\n",
    "#     medyear = properties17.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n",
    "#     meanarea = properties17.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n",
    "#     medlat = properties17.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n",
    "#     medlong = properties17.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n",
    "\n",
    "#     test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "#                         properties17.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "#                         how = 'left', on = 'ParcelId' )\n",
    "#     if USE_SEASONAL_FEATURES:\n",
    "#         test_df['transactiondate'] = '2017-10-31'\n",
    "#     calculate_features(test_df)\n",
    "#     x_test17 = test_df.drop(dropvars+droptest, axis=1)\n",
    "\n",
    "    del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {  # best as of 2017-09-28 13:20 UTC\n",
    "    'eta': LEARNING_RATE,\n",
    "    'max_depth': 7, \n",
    "    'subsample': 0.6,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 5.0,\n",
    "    'alpha': 0.65,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'base_score': y_mean,'taxdelinquencyyear'\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dvalid_x = xgb.DMatrix(x_valid)\n",
    "dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n",
    "if not CV_ONLY:\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "#     dtest17 = xgb.DMatrix(x_test17)\n",
    "#     del x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting rounds: 2857.0\n",
      "Early stoping rounds: 143.0\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\n",
    "early_stopping_rounds = round( num_boost_rounds / 20 )\n",
    "print('Boosting rounds: {}'.format(num_boost_rounds))\n",
    "print('Early stoping rounds: {}'.format(early_stopping_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2857.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_boost_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053445\teval-mae:0.065273\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 143.0 rounds.\n",
      "[100]\ttrain-mae:0.052722\teval-mae:0.064775\n",
      "[200]\ttrain-mae:0.052269\teval-mae:0.064542\n",
      "[300]\ttrain-mae:0.051934\teval-mae:0.064416\n",
      "[400]\ttrain-mae:0.051687\teval-mae:0.064353\n",
      "[500]\ttrain-mae:0.051454\teval-mae:0.064314\n",
      "[600]\ttrain-mae:0.051238\teval-mae:0.064283\n",
      "[700]\ttrain-mae:0.051049\teval-mae:0.064273\n",
      "[800]\ttrain-mae:0.050861\teval-mae:0.064261\n",
      "[900]\ttrain-mae:0.050691\teval-mae:0.064266\n",
      "Stopping. Best iteration:\n",
      "[804]\ttrain-mae:0.050853\teval-mae:0.064258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round=2875,\n",
    "                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n",
    "                  verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost validation set predictions:\n",
      "          0\n",
      "0  0.001607\n",
      "1  0.022175\n",
      "2  0.020656\n",
      "3  0.012385\n",
      "4  0.019222\n",
      "\n",
      "Mean absolute validation error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.064258203"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n",
    "print( \"XGBoost validation set predictions:\" )\n",
    "print( pd.DataFrame(valid_pred).head() )\n",
    "print(\"\\nMean absolute validation error:\")\n",
    "mean_absolute_error(y_valid, valid_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217, 65)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "print test_pred.shape\n",
    "print submit.shape\n",
    "for c in submit.columns:\n",
    "    if c != 'ParcelId':\n",
    "        submit[c] = test_pred\n",
    "print submit.head(100)\n",
    "submit.to_csv('../data/feature_65.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "?QuantReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZE_FUDGE_FACTOR:\n",
    "    mod = QuantReg(y_valid, valid_pred)\n",
    "    res = mod.fit(q=.5)\n",
    "    print(\"\\nLAD Fit for Fudge Factor:\")\n",
    "    print(res.summary())\n",
    "\n",
    "    fudge = res.params[0]\n",
    "    print(\"Optimized fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "\n",
    "    fudge **= FUDGE_FACTOR_SCALEDOWN\n",
    "    print(\"Scaled down fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "else:\n",
    "    fudge=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053266\n",
      "[10]\ttrain-mae:0.053176\n",
      "[20]\ttrain-mae:0.053092\n",
      "[30]\ttrain-mae:0.053007\n",
      "[40]\ttrain-mae:0.052932\n",
      "[50]\ttrain-mae:0.052855\n",
      "[60]\ttrain-mae:0.052788\n",
      "[70]\ttrain-mae:0.052725\n",
      "[80]\ttrain-mae:0.052671\n",
      "[90]\ttrain-mae:0.052615\n",
      "[100]\ttrain-mae:0.052562\n",
      "[110]\ttrain-mae:0.05251\n",
      "[120]\ttrain-mae:0.052461\n",
      "[130]\ttrain-mae:0.052414\n",
      "[140]\ttrain-mae:0.052365\n",
      "[150]\ttrain-mae:0.052319\n",
      "[160]\ttrain-mae:0.05228\n",
      "[170]\ttrain-mae:0.052237\n",
      "[180]\ttrain-mae:0.052199\n",
      "[190]\ttrain-mae:0.052164\n",
      "[200]\ttrain-mae:0.052128\n",
      "[210]\ttrain-mae:0.052095\n",
      "[220]\ttrain-mae:0.052058\n",
      "[230]\ttrain-mae:0.052024\n",
      "[240]\ttrain-mae:0.051993\n",
      "[250]\ttrain-mae:0.051962\n",
      "[260]\ttrain-mae:0.051933\n",
      "[270]\ttrain-mae:0.051905\n",
      "[280]\ttrain-mae:0.051878\n",
      "[290]\ttrain-mae:0.051849\n",
      "[300]\ttrain-mae:0.051818\n",
      "[310]\ttrain-mae:0.051792\n",
      "[320]\ttrain-mae:0.051766\n",
      "[330]\ttrain-mae:0.051741\n",
      "[340]\ttrain-mae:0.051716\n",
      "[350]\ttrain-mae:0.051689\n",
      "[360]\ttrain-mae:0.051663\n",
      "[370]\ttrain-mae:0.051638\n",
      "[380]\ttrain-mae:0.051614\n",
      "[390]\ttrain-mae:0.051592\n",
      "[400]\ttrain-mae:0.051569\n",
      "[410]\ttrain-mae:0.051546\n",
      "[420]\ttrain-mae:0.05152\n",
      "[430]\ttrain-mae:0.051501\n",
      "[440]\ttrain-mae:0.051478\n",
      "[450]\ttrain-mae:0.051459\n",
      "[460]\ttrain-mae:0.051437\n",
      "[470]\ttrain-mae:0.051421\n",
      "[480]\ttrain-mae:0.051399\n",
      "[490]\ttrain-mae:0.051379\n",
      "[500]\ttrain-mae:0.051355\n",
      "[510]\ttrain-mae:0.051338\n",
      "[520]\ttrain-mae:0.051319\n",
      "[530]\ttrain-mae:0.0513\n",
      "[540]\ttrain-mae:0.051281\n",
      "[550]\ttrain-mae:0.051266\n",
      "[560]\ttrain-mae:0.051247\n",
      "[570]\ttrain-mae:0.05123\n",
      "[580]\ttrain-mae:0.051212\n",
      "[590]\ttrain-mae:0.051191\n",
      "[600]\ttrain-mae:0.05117\n",
      "[610]\ttrain-mae:0.051155\n",
      "[620]\ttrain-mae:0.051139\n",
      "[630]\ttrain-mae:0.051121\n",
      "[640]\ttrain-mae:0.0511\n",
      "[650]\ttrain-mae:0.051082\n",
      "[660]\ttrain-mae:0.051065\n",
      "[670]\ttrain-mae:0.051049\n",
      "[680]\ttrain-mae:0.051033\n",
      "[690]\ttrain-mae:0.051019\n",
      "[700]\ttrain-mae:0.051001\n",
      "[710]\ttrain-mae:0.050985\n",
      "[720]\ttrain-mae:0.050967\n",
      "[730]\ttrain-mae:0.050952\n",
      "[740]\ttrain-mae:0.050933\n",
      "[750]\ttrain-mae:0.050918\n",
      "[760]\ttrain-mae:0.050902\n",
      "[770]\ttrain-mae:0.050884\n",
      "[780]\ttrain-mae:0.050868\n",
      "[790]\ttrain-mae:0.050852\n",
      "[800]\ttrain-mae:0.050838\n",
      "[810]\ttrain-mae:0.050823\n",
      "[820]\ttrain-mae:0.050807\n",
      "[830]\ttrain-mae:0.050791\n",
      "[840]\ttrain-mae:0.050777\n",
      "[850]\ttrain-mae:0.050759\n",
      "[860]\ttrain-mae:0.050744\n",
      "[870]\ttrain-mae:0.050728\n",
      "[880]\ttrain-mae:0.050714\n",
      "[890]\ttrain-mae:0.050698\n",
      "[900]\ttrain-mae:0.050684\n",
      "[910]\ttrain-mae:0.05067\n",
      "[920]\ttrain-mae:0.050655\n",
      "[930]\ttrain-mae:0.050641\n",
      "[940]\ttrain-mae:0.050627\n",
      "[950]\ttrain-mae:0.050612\n"
     ]
    }
   ],
   "source": [
    "if FIT_FULL_TRAIN_SET and not CV_ONLY:\n",
    "    if not FIT_COMBINED_TRAIN_SET:\n",
    "        # Merge 2016 and 2017 data sets\n",
    "        train16 = pd.read_csv('../input/train_2016_v2.csv')\n",
    "        train17 = pd.read_csv('../input/train_2017.csv')\n",
    "        train16 = pd.merge(train16, properties16, how = 'left', on = 'parcelid')\n",
    "        train17 = pd.merge(train17, properties17, how = 'left', on = 'parcelid')\n",
    "        train17[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n",
    "        train_df = pd.concat([train16, train17], axis = 0)\n",
    "        # Generate features\n",
    "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n",
    "        calculate_features(train_df)\n",
    "        # Remove outliers\n",
    "        train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "        train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "        # Create final training data sets\n",
    "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n",
    "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n",
    "        n_full = x_full.shape[0]     \n",
    "    elif FIT_2017_TRAIN_SET:\n",
    "        train = pd.read_csv('../data/train_2017.csv')\n",
    "        train_df = train.merge(properties17, how='left', on='parcelid')\n",
    "        # Generate features\n",
    "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n",
    "        calculate_features(train_df)\n",
    "        # Remove outliers\n",
    "        train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "        train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "        # Create final training data sets\n",
    "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n",
    "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n",
    "        n_full = x_full.shape[0]     \n",
    "    dtrain = xgb.DMatrix(x_full, y_full)\n",
    "    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)\n",
    "    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n",
    "                           evals=[(dtrain,'train')], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del properties16\n",
    "del properties17\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost test set predictions for 2016:\n",
      "          0\n",
      "0  0.002543\n",
      "1 -0.006614\n",
      "2  0.025026\n",
      "3  0.044890\n",
      "4 -0.002124\n",
      "XGBoost test set predictions for 2017:\n",
      "          0\n",
      "0  0.002543\n",
      "1 -0.006614\n",
      "2  0.031241\n",
      "3  0.045232\n",
      "4 -0.001447\n"
     ]
    }
   ],
   "source": [
    "if not CV_ONLY:\n",
    "    if FIT_FULL_TRAIN_SET:\n",
    "        pred = fudge*full_model.predict(dtest)\n",
    "        pred17 = fudge*full_model.predict(dtest17)\n",
    "    else:\n",
    "        pred = fudge*model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "        pred17 = fudge*model.predict(dtest17, ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "    print( \"XGBoost test set predictions for 2016:\" )\n",
    "    print( pd.DataFrame(pred).head() )\n",
    "    print( \"XGBoost test set predictions for 2017:\" )\n",
    "    print( pd.DataFrame(pred17).head() )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d8e8fca2dc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process properties for 2016\n",
    "# test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "#                     properties16.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "#                     how = 'left', on = 'ParcelId' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test: (2985217, 65)\n",
      "Shape test: (2985217, 65)\n",
      "Shape test: (2985217, 65)\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                        how = 'left', on = 'ParcelId' )\n",
    "test_df['transactiondate'] = '2016-10-31'\n",
    "calculate_features(test_df)\n",
    "x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "print('Shape test: {}'.format(x_test.shape))\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "test_10 = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "###\n",
    "test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                        how = 'left', on = 'ParcelId' )\n",
    "test_df['transactiondate'] = '2016-11-30'\n",
    "calculate_features(test_df)\n",
    "x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "print('Shape test: {}'.format(x_test.shape))\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "test_11 = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "###\n",
    "test_df = pd.merge( sample_submission[['ParcelId']], \n",
    "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                        how = 'left', on = 'ParcelId' )\n",
    "test_df['transactiondate'] = '2016-12-31'\n",
    "calculate_features(test_df)\n",
    "x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "print('Shape test: {}'.format(x_test.shape))\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "test_12 = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n",
    "           '201610': test_10, '201611': test_11, '201612': test_12,\n",
    "           '201710': test_10, '201711': test_11, '201712': test_12})\n",
    "output.to_csv('../data/64258_with_time.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParcelId</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>decktypeid</th>\n",
       "      <th>...</th>\n",
       "      <th>N-GarPoolAC</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>med_year</th>\n",
       "      <th>med_lat</th>\n",
       "      <th>med_long</th>\n",
       "      <th>zip_std</th>\n",
       "      <th>city_std</th>\n",
       "      <th>hood_std</th>\n",
       "      <th>cos_season</th>\n",
       "      <th>sin_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096225</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.154446</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>-0.242687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096225</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.154446</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>-0.242687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143986</td>\n",
       "      <td>0.140596</td>\n",
       "      <td>0.154446</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>-0.242687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1989.878889</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>34156200.0</td>\n",
       "      <td>-118444765.0</td>\n",
       "      <td>0.128849</td>\n",
       "      <td>0.195053</td>\n",
       "      <td>0.151713</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>-0.242687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1530.869588</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>34179126.0</td>\n",
       "      <td>-118374599.0</td>\n",
       "      <td>0.143689</td>\n",
       "      <td>0.195053</td>\n",
       "      <td>0.221739</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>-0.242687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ParcelId  airconditioningtypeid  architecturalstyletypeid  basementsqft  \\\n",
       "0  10754147                   -1.0                      -1.0          -1.0   \n",
       "1  10759547                   -1.0                      -1.0          -1.0   \n",
       "2  10843547                   -1.0                      -1.0          -1.0   \n",
       "3  10859147                   -1.0                      -1.0          -1.0   \n",
       "4  10879947                   -1.0                      -1.0          -1.0   \n",
       "\n",
       "   bathroomcnt  bedroomcnt  buildingclasstypeid  buildingqualitytypeid  \\\n",
       "0          0.0         0.0                 -1.0                   -1.0   \n",
       "1          0.0         0.0                 -1.0                   -1.0   \n",
       "2          0.0         0.0                 -1.0                   -1.0   \n",
       "3          0.0         0.0                  3.0                    7.0   \n",
       "4          0.0         0.0                  4.0                   -1.0   \n",
       "\n",
       "   calculatedbathnbr  decktypeid     ...      N-GarPoolAC    mean_area  \\\n",
       "0               -1.0        -1.0     ...                0          NaN   \n",
       "1               -1.0        -1.0     ...                0          NaN   \n",
       "2               -1.0        -1.0     ...                0          NaN   \n",
       "3               -1.0        -1.0     ...                0  1989.878889   \n",
       "4               -1.0        -1.0     ...                0  1530.869588   \n",
       "\n",
       "   med_year     med_lat     med_long   zip_std  city_std  hood_std  \\\n",
       "0       NaN         NaN          NaN  0.096225  0.104574  0.154446   \n",
       "1       NaN         NaN          NaN  0.096225  0.104574  0.154446   \n",
       "2       NaN         NaN          NaN  0.143986  0.140596  0.154446   \n",
       "3    1955.0  34156200.0 -118444765.0  0.128849  0.195053  0.151713   \n",
       "4    1947.0  34179126.0 -118374599.0  0.143689  0.195053  0.221739   \n",
       "\n",
       "   cos_season  sin_season  \n",
       "0    0.970105   -0.242687  \n",
       "1    0.970105   -0.242687  \n",
       "2    0.970105   -0.242687  \n",
       "3    0.970105   -0.242687  \n",
       "4    0.970105   -0.242687  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean absolute validation error without fudge factor: ',)\n",
      "0.0642582\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute validation error without fudge factor: \", )\n",
    "print( mean_absolute_error(y_valid, valid_pred) )\n",
    "if OPTIMIZE_FUDGE_FACTOR:\n",
    "    print(\"Mean absolute validation error with fudge factor:\")\n",
    "    print( mean_absolute_error(y_valid, fudge*valid_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
